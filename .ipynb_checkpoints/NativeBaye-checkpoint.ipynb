{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preproccess functionï¼štext -> token and word vector\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "__tokenization_pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "tokenizer = nltk.tokenize.regexp.RegexpTokenizer(__tokenization_pattern)\n",
    "\n",
    "def preprocessor(text):\n",
    "    stems = []\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    for token in tokens:\n",
    "        if token.isalpha() and token not in stopwords_en:\n",
    "            stems.append(str(stemmer.stem(token)))\n",
    "    return stems\n",
    "\n",
    "bow_vectorizer = CountVectorizer(lowercase = False, \n",
    "                                 tokenizer = lambda x: x, # because we already have tokens available\n",
    "                                 stop_words = None, ## stop words removal already done from NLTK\n",
    "                                 max_features = 5000, ## pick top 5K words by frequency\n",
    "                                 ngram_range = (1, 1), ## we want unigrams now\n",
    "                                 binary = False) ## we want as binary/boolean features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text from files and proccess them to word vector\n",
    "path_base = 'dataset/'\n",
    "path_years = ['2014/', '2015/', '2016/']\n",
    "path_category = 'category'\n",
    "\n",
    "token = list()\n",
    "x = list()\n",
    "y = list()\n",
    "\n",
    "for year in path_years:\n",
    "    for category in ['1', '2']:\n",
    "        path = path_base + year + path_category + category +'/'\n",
    "        for filename in os.listdir(path):\n",
    "            with open (path + filename, \"r\") as f:\n",
    "                text = f.read().replace(u'\\xa0', ' ').replace('\\n', ' ')\n",
    "                token.append(preprocessor(text))\n",
    "                y.append(category)\n",
    "text_vec = bow_vectorizer.fit_transform(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891 document\n"
     ]
    }
   ],
   "source": [
    "print(len(y), 'documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.6946902654867256\n",
      "accuracy = 0.6611570247933884\n",
      "accuracy = 0.7027027027027027\n",
      "accuracy = 0.6457399103139013\n",
      "accuracy = 0.690677966101695\n",
      "accuracy = 0.6946902654867256\n",
      "accuracy = 0.6363636363636364\n",
      "accuracy = 0.6308411214953271\n",
      "accuracy = 0.6292682926829268\n",
      "accuracy = 0.673728813559322\n",
      "accuracy = 0.6325581395348837\n",
      "accuracy = 0.6697247706422018\n",
      "accuracy = 0.6483050847457628\n",
      "accuracy = 0.6983471074380165\n",
      "accuracy = 0.7103825136612022\n",
      "accuracy = 0.6238938053097345\n",
      "accuracy = 0.645\n",
      "accuracy = 0.7090909090909091\n",
      "accuracy = 0.6093023255813953\n",
      "accuracy = 0.6504065040650406\n",
      "accuracy = 0.6425339366515838\n",
      "accuracy = 0.6267942583732058\n",
      "accuracy = 0.6637554585152838\n",
      "accuracy = 0.646551724137931\n",
      "accuracy = 0.6796116504854369\n",
      "accuracy = 0.6697247706422018\n",
      "accuracy = 0.6303317535545023\n",
      "accuracy = 0.6666666666666666\n",
      "accuracy = 0.6583333333333333\n",
      "accuracy = 0.6779661016949152\n",
      "accuracy = 0.5964125560538116\n",
      "accuracy = 0.6766169154228856\n",
      "accuracy = 0.6198347107438017\n",
      "accuracy = 0.6666666666666666\n",
      "accuracy = 0.6354679802955665\n",
      "accuracy = 0.6495327102803738\n",
      "accuracy = 0.6944444444444444\n",
      "accuracy = 0.6416666666666667\n",
      "accuracy = 0.6607929515418502\n",
      "accuracy = 0.71\n",
      "accuracy = 0.6615384615384615\n",
      "accuracy = 0.6787330316742082\n",
      "accuracy = 0.6342592592592593\n",
      "accuracy = 0.6130434782608696\n",
      "accuracy = 0.625\n",
      "accuracy = 0.6780487804878049\n",
      "accuracy = 0.6488888888888888\n",
      "accuracy = 0.6380090497737556\n",
      "accuracy = 0.6724890829694323\n",
      "accuracy = 0.6324786324786325\n",
      "\n",
      "\n",
      "\n",
      "average accuracy = 0.6570613016111588\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "sum = 0\n",
    "for _ in range(50):\n",
    "    # Split the dataset to train set and test set\n",
    "    msk = np.random.rand(len(y)) < 0.75\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "\n",
    "    train_x = text_vec[msk]\n",
    "    test_x = text_vec[~msk]\n",
    "\n",
    "    y = le.fit_transform(y)\n",
    "    train_y = y[msk]\n",
    "    test_y = y[~msk]\n",
    "    \n",
    "    # Train with MultinomialNB\n",
    "    \n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(train_x, train_y)\n",
    "    \n",
    "    # Get prediction\n",
    "    preds_bow = classifier.predict(test_x)\n",
    "    to_print = [le.inverse_transform(pred) for pred in preds_bow ]\n",
    "    # print(to_print)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    confusion = confusion_matrix(test_y, preds_bow)\n",
    "    acc_bow = accuracy_score(test_y, preds_bow)\n",
    "    precisions_bow, recalls_bow, f1_scores_bow, _ = precision_recall_fscore_support(test_y, preds_bow)\n",
    "    sum += acc_bow\n",
    "    print(\"accuracy = {}\".format(acc_bow))\n",
    "print(\"\\n\\n\\naverage accuracy = {}\".format(sum / 50))    \n",
    "#     print(\"{:>25} {:>4} {:>4} {:>4}\".format(\"\", \"prec\", \"rec\", \"F1\"))\n",
    "#     for (idx, scores) in enumerate(zip(precisions_bow, recalls_bow, f1_scores_bow)):\n",
    "#         print(\"{:>25} {:.2f} {:.2f} {:.2f}\".format(\n",
    "#             le.inverse_transform(idx), scores[0], scores[1], scores[2]\n",
    "#         ))\n",
    "#     print('confusion matrix:\\n{}'.format( confusion) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
