{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "path_base = 'dataset/2014/category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preproccess functionï¼štext -> token and word vector\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "__tokenization_pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "tokenizer = nltk.tokenize.regexp.RegexpTokenizer(__tokenization_pattern)\n",
    "\n",
    "def preprocessor(text):\n",
    "    stems = []\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    for token in tokens:\n",
    "        if token.isalpha() and token not in stopwords_en:\n",
    "            stems.append(str(stemmer.stem(token)))\n",
    "    return stems\n",
    "\n",
    "bow_vectorizer = CountVectorizer(lowercase = False, \n",
    "                                 tokenizer = lambda x: x, # because we already have tokens available\n",
    "                                 stop_words = None, ## stop words removal already done from NLTK\n",
    "                                 max_features = 5000, ## pick top 5K words by frequency\n",
    "                                 ngram_range = (1, 1), ## we want unigrams now\n",
    "                                 binary = False) ## we want as binary/boolean features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text from files and proccess them to word vector\n",
    "token = list()\n",
    "x = list()\n",
    "y = list()\n",
    "\n",
    "for category in ['1', '2']:\n",
    "    path = path_base + category +'/'\n",
    "    for filename in os.listdir(path):\n",
    "        with open (path + filename, \"r\") as f:\n",
    "            text = f.read().replace(u'\\xa0', ' ').replace('\\n', ' ')\n",
    "            token.append(preprocessor(text))\n",
    "            y.append(category)\n",
    "text_vec = bow_vectorizer.fit_transform(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7121212121212122\n",
      "accuracy = 0.6923076923076923\n",
      "accuracy = 0.7230769230769231\n",
      "accuracy = 0.6923076923076923\n",
      "accuracy = 0.6666666666666666\n",
      "accuracy = 0.6835443037974683\n",
      "accuracy = 0.625\n",
      "accuracy = 0.7592592592592593\n",
      "accuracy = 0.8166666666666667\n",
      "accuracy = 0.6461538461538462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "for _ in range(10):\n",
    "    # Split the dataset to train set and test set\n",
    "    msk = np.random.rand(len(y)) < 0.75\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "\n",
    "    train_x = text_vec[msk]\n",
    "    test_x = text_vec[~msk]\n",
    "\n",
    "    y = le.fit_transform(y)\n",
    "    train_y = y[msk]\n",
    "    test_y = y[~msk]\n",
    "    \n",
    "    # Train with MultinomialNB\n",
    "    \n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(train_x, train_y)\n",
    "    \n",
    "    # Get prediction\n",
    "    preds_bow = classifier.predict(test_x)\n",
    "    to_print = [le.inverse_transform(pred) for pred in preds_bow ]\n",
    "    # print(to_print)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    confusion = confusion_matrix(test_y, preds_bow)\n",
    "    acc_bow = accuracy_score(test_y, preds_bow)\n",
    "    precisions_bow, recalls_bow, f1_scores_bow, _ = precision_recall_fscore_support(test_y, preds_bow)\n",
    "\n",
    "    print(\"accuracy = {}\".format(acc_bow))\n",
    "#     print(\"{:>25} {:>4} {:>4} {:>4}\".format(\"\", \"prec\", \"rec\", \"F1\"))\n",
    "#     for (idx, scores) in enumerate(zip(precisions_bow, recalls_bow, f1_scores_bow)):\n",
    "#         print(\"{:>25} {:.2f} {:.2f} {:.2f}\".format(\n",
    "#             le.inverse_transform(idx), scores[0], scores[1], scores[2]\n",
    "#         ))\n",
    "#     print('confusion matrix:\\n{}'.format( confusion) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
